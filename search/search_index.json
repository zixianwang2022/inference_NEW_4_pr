{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLPerf Inference Benchmarks","text":""},{"location":"#overview","title":"Overview","text":"<p>The currently valid MLPerf Inference Benchmarks as of MLPerf inference v4.0 round are listed below, categorized by tasks. Under each model you can find its details like the dataset used, reference accuracy, server latency constraints etc.</p>"},{"location":"#image-classification","title":"Image Classification","text":""},{"location":"#resnet50-v15","title":"ResNet50-v1.5","text":"<ul> <li>Dataset: Imagenet-2012 (224x224) Validation<ul> <li>Dataset Size: 50,000</li> <li>QSL Size: 1,024</li> </ul> </li> <li>Number of Parameters: 25.6 million</li> <li>FLOPs: 3.8 billion</li> <li>Reference Model Accuracy: 76.46% ACC</li> <li>Server Scenario Latency Constraint: 15ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#text-to-image","title":"Text to Image","text":""},{"location":"#stable-diffusion","title":"Stable Diffusion","text":"<ul> <li>Dataset: Subset of Coco2014<ul> <li>Dataset Size: 5,000</li> <li>QSL Size: 5,000</li> </ul> </li> <li>Number of Parameters: 3.5 billion </li> <li>FLOPs: 1.28 - 2.4 trillion</li> <li>Reference Model Accuracy (fp32):  CLIP: 31.74981837, FID: 23.48046692</li> <li>Required Accuracy (Closed Division):<ul> <li>CLIP: 31.68631873 \u2264 CLIP \u2264 31.81331801 (within 0.2% of the reference model CLIP score)</li> <li>FID: 23.01085758 \u2264 FID \u2264 23.95007626 (within 2% of the reference model FID score)</li> </ul> </li> <li>Equal Issue mode: False</li> <li>High accuracy variant: No</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#object-detection","title":"Object Detection","text":""},{"location":"#retinanet","title":"Retinanet","text":"<ul> <li>Dataset: OpenImages<ul> <li>Dataset Size: 24,781</li> <li>QSL Size: 64</li> </ul> </li> <li>Number of Parameters: TBD</li> <li>Reference Model Accuracy (fp32) : 0.3755 mAP</li> <li>Server Scenario Latency Constraint: 100ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#medical-image-segmentation","title":"Medical Image Segmentation","text":""},{"location":"#3d-unet","title":"3d-unet","text":"<ul> <li>Dataset: KiTS2019<ul> <li>Dataset Size: 42</li> <li>QSL Size: 42</li> </ul> </li> <li>Number of Parameters: 32.5 million</li> <li>FLOPs: 100-300 billion</li> <li>Reference Model Accuracy (fp32) : 0.86330 Mean DICE Score</li> <li>Server Scenario: Not Applicable</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#language-tasks","title":"Language Tasks","text":""},{"location":"#question-answering","title":"Question Answering","text":""},{"location":"#bert-large","title":"Bert-Large","text":"<ul> <li>Dataset: Squad v1.1 (384 Sequence Length)<ul> <li>Dataset Size: 10,833</li> <li>QSL Size: 10,833</li> </ul> </li> <li>Number of Parameters: 340 million </li> <li>FLOPs: ~128 billion</li> <li>Reference Model Accuracy (fp32) : F1 Score = 90.874%</li> <li>Server Scenario Latency Constraint: 130ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#llama2-70b","title":"LLAMA2-70B","text":"<ul> <li>Dataset: OpenORCA (GPT-4 split, max_seq_len=1024)<ul> <li>Dataset Size: 24,576</li> <li>QSL Size: 24,576</li> </ul> </li> <li>Number of Parameters: 70 billion</li> <li>FLOPs: ~500 trillion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 44.4312</li> <li>Rouge2: 22.0352</li> <li>RougeL: 28.6162</li> <li>Tokens_per_sample: 294.45</li> </ul> </li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#text-summarization","title":"Text Summarization","text":""},{"location":"#gpt-j","title":"GPT-J","text":"<ul> <li>Dataset: CNN Daily Mail v3.0.0<ul> <li>Dataset Size: 13,368</li> <li>QSL Size: 13,368</li> </ul> </li> <li>Number of Parameters: 6 billion</li> <li>FLOPs: ~148 billion</li> <li>Reference Model Accuracy (fp32) :<ul> <li>Rouge1: 42.9865</li> <li>Rouge2: 20.1235</li> <li>RougeL: 29.9881</li> <li>Gen_len: 4,016,878</li> </ul> </li> <li>Server Scenario Latency Constraint: 20s</li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter, Edge</li> </ul>"},{"location":"#mixed-tasks-question-answering-math-and-code-generation","title":"Mixed Tasks (Question Answering, Math, and Code Generation)","text":""},{"location":"#mixtral-8x7b","title":"Mixtral-8x7B","text":"<ul> <li>Datasets:<ul> <li>OpenORCA (5k samples of GPT-4 split, max_seq_len=2048)</li> <li>GSM8K (5k samples of the validation split, max_seq_len=2048)</li> <li>MBXP (5k samples of the validation split, max_seq_len=2048)</li> <li>Dataset Size: 15,000</li> <li>QSL Size: 15,000</li> </ul> </li> <li>Number of Parameters: 47 billion </li> <li>Reference Model Accuracy (fp16) :<ul> <li>OpenORCA<ul> <li>Rouge1: 45.4911</li> <li>Rouge2: 23.2829</li> <li>RougeL: 30.3615</li> </ul> </li> <li>GSM8K Accuracy: 73.78%</li> <li>MBXP Accuracy: 60.12%</li> </ul> </li> <li>Tokens_per_sample: 294.45</li> <li>Server Scenario Latency Constraint:<ul> <li>TTFT: 2000ms</li> <li>TPOT: 200ms</li> </ul> </li> <li>Equal Issue mode: True</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#recommendation","title":"Recommendation","text":""},{"location":"#dlrm_v2","title":"DLRM_v2","text":"<ul> <li>Dataset: Synthetic Multihot Criteo<ul> <li>Dataset Size: 204,800</li> <li>QSL Size: 204,800</li> </ul> </li> <li>Number of Parameters: ~23 billion</li> <li>Reference Model Accuracy: AUC = 80.31%</li> <li>Server Scenario Latency Constraint: 60ms</li> <li>Equal Issue mode: False</li> <li>High accuracy variant: Yes</li> <li>Submission Category: Datacenter</li> </ul>"},{"location":"#submission-categories","title":"Submission Categories","text":"<ul> <li>Datacenter Category: All the current inference benchmarks are applicable to the datacenter category.</li> <li>Edge Category: All benchmarks except DLRMv2, LLAMA2-70B, and Mixtral-8x7B are applicable to the edge category.</li> </ul>"},{"location":"#high-accuracy-variants","title":"High Accuracy Variants","text":"<ul> <li>Benchmarks: <code>bert</code>, <code>llama2-70b</code>, <code>gpt-j</code>,  <code>dlrm_v2</code>, and <code>3d-unet</code> have a normal accuracy variant as well as a high accuracy variant.</li> <li>Requirement: Must achieve at least 99.9% of the reference model accuracy, compared to the default 99% accuracy requirement.</li> </ul>"},{"location":"index_gh/","title":"MLPerf\u2122 Inference Benchmark Suite","text":"<p>MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios. </p> <p>Please see the MLPerf Inference benchmark paper for a detailed description of the benchmarks along with the motivation and guiding principles behind the benchmark suite. If you use any part of this benchmark (e.g., reference implementations, submissions, etc.), please cite the following:</p> <p><pre><code>@misc{reddi2019mlperf,\n    title={MLPerf Inference Benchmark},\n    author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},\n    year={2019},\n    eprint={1911.02549},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre> Please see here for the MLPerf inference documentation website which includes automated commands to run MLPerf inference benchmarks using different implementations.</p>"},{"location":"index_gh/#mlperf-inference-v41-submission-deadline-july-26-2024","title":"MLPerf Inference v4.1 (submission deadline July 26, 2024)","text":"<p>For submissions, please use the master branch and any commit since the 4.1 seed release although it is best to use the latest commit. v4.1 tag will be created from the master branch after the result publication.</p> <p>For power submissions please use SPEC PTD 1.10 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter mixtral-8x7b language/mixtral-8x7b pytorch OpenOrca, MBXP, GSM8K datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v40-submission-february-23-2024","title":"MLPerf Inference v4.0 (submission February 23, 2024)","text":"<p>There is an extra one-week extension allowed only for the llama2-70b submissions. For submissions, please use the master branch and any commit since the 4.0 seed release although it is best to use the latest commit. v4.0 tag will be created from the master branch after the result publication.</p> <p>For power submissions please use SPEC PTD 1.10 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"index_gh/#mlperf-inference-v31-submission-august-18-2023","title":"MLPerf Inference v3.1 (submission August 18, 2023)","text":"<p>Please use v3.1 tag (<code>git checkout v3.1</code>) if you would like to reproduce the v3.1 results. </p> <p>For reproducing power submissions please use the <code>master</code> branch of the MLCommons power-dev repository and checkout to e9e16b1299ef61a2a5d8b9abf5d759309293c440. </p> <p>You can see the individual README files in the benchmark task folders for more details regarding the benchmarks. For reproducing the submitted results please see the README files under the respective submitter folders in the inference v3.1 results repository.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter"},{"location":"index_gh/#mlperf-inference-v30-submission-03032023","title":"MLPerf Inference v3.0 (submission 03/03/2023)","text":"<p>Please use the v3.0 tag (<code>git checkout v3.0</code>) if you would like to reproduce v3.0 results.</p> <p>You can see the individual Readme files in the reference app for more details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v21-submission-08052022","title":"MLPerf Inference v2.1 (submission 08/05/2022)","text":"<p>Use the r2.1 branch (<code>git checkout r2.1</code>) if you want to submit or reproduce v2.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v20-submission-02252022","title":"MLPerf Inference v2.0 (submission 02/25/2022)","text":"<p>Use the r2.0 branch (<code>git checkout r2.0</code>) if you want to submit or reproduce v2.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v11-submission-08132021","title":"MLPerf Inference v1.1 (submission 08/13/2021)","text":"<p>Use the r1.1 branch (<code>git checkout r1.1</code>) if you want to submit or reproduce v1.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v10-submission-03192021","title":"MLPerf Inference v1.0 (submission 03/19/2021)","text":"<p>Use the r1.0 branch (<code>git checkout r1.0</code>) if you want to submit or reproduce v1.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow(?) Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"index_gh/#mlperf-inference-v07-submission-9182020","title":"MLPerf Inference v0.7 (submission 9/18/2020)","text":"<p>Use the r0.7 branch (<code>git checkout r0.7</code>) if you want to submit or reproduce v0.7 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 vision/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 bert language/bert tensorflow, pytorch, onnx squad-1.1 dlrm recommendation/dlrm pytorch, tensorflow(?), onnx(?) Criteo Terabyte 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus"},{"location":"index_gh/#mlperf-inference-v05","title":"MLPerf Inference v0.5","text":"<p>Use the r0.5 branch (<code>git checkout r0.5</code>) if you want to reproduce v0.5 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 mobilenet-v1 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 gnmt v0.5/translation/gnmt/ tensorflow, pytorch See Readme"},{"location":"benchmarks/image_classification/get-resnet50-data/","title":"Image Classification using ResNet50","text":""},{"location":"benchmarks/image_classification/get-resnet50-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>ResNet50 validation run uses the Imagenet 2012 validation dataset consisting of 50,000 images.</p> <p>ResNet50 calibration dataset consist of 500 images selected from the Imagenet 2012 validation dataset. There are 2 alternative options for the calibration dataset.</p>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,imagenet,validation -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-1","title":"Get Calibration Dataset Using Option 1","text":"<pre><code>cm run script --tags=get,dataset,imagenet,calibration,_mlperf.option1 -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#get-calibration-dataset-using-option-2","title":"Get Calibration Dataset Using Option 2","text":"<pre><code>cm run script --tags=get,dataset,imagenet,calibration,_mlperf.option2 -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf ResNet50 Model</p> TensorflowOnnx"},{"location":"benchmarks/image_classification/get-resnet50-data/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,resnet50,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/image_classification/get-resnet50-data/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,resnet50,_onnx -j\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/","title":"Image Classification using Mobilenet models","text":"<p>Install CM following the installation page.</p> <p>Mobilenet models are not official MLPerf models and so cannot be used for a Closed division MLPerf inference submission. But since they can be run with Imagenet dataset, we are allowed to use them for Open division submission. Only CPU runs are supported now. </p>"},{"location":"benchmarks/image_classification/mobilenets/#tflite-backend","title":"TFLite Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V2MobilenetsEfficientnet"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1","title":"Mobilenet V1","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2","title":"Mobilenet V2","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2_1","title":"Mobilenet V2","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3","title":"Mobilenet V1,V2,V3","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#efficientnet","title":"Efficientnet","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#armnn-backend","title":"ARMNN Backend","text":"Mobilenet-V1Mobilenet-V2Mobilenet-V2MobilenetsEfficientnet"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1_1","title":"Mobilenet V1","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_armnn,_mobilenet-v1 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2_2","title":"Mobilenet V2","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v2_3","title":"Mobilenet V2","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_armnn,_mobilenet-v2 \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#mobilenet-v1v2v3_1","title":"Mobilenet V1,V2,V3","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_armnn,_mobilenet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/mobilenets/#efficientnet_1","title":"Efficientnet","text":"<pre><code>cm run script --tags=run,mobilenet-models,_tflite,_armnn,_efficientnet \\\n --adr.compiler.tags=gcc\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/","title":"Image Classification using ResNet50","text":"MLCommons-PythonNvidiaIntelQualcommMLCommons-C++"},{"location":"benchmarks/image_classification/resnet50/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RESNET50</p> edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_1","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_2","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_3","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_4","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_5","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_6","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_7","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_8","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_8","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_9","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_9","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_10","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_10","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_11","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_11","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework_1","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_18","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_19","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_20","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_12","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_21","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_22","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_13","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_13","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for resnet50 you can follow this README.</li> <li>Please see mobilenets.md for running mobilenet models for Image Classification.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_23","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RESNET50</p> edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_1","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_10","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_24","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_12","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_12","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_11","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_25","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>RESNET50</p> edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_2","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/image_classification/resnet50/#pytorch-framework","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_12","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_26","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_13","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_13","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_14","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_14","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_27","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_14","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_14","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/image_classification/resnet50/#pytorch-framework_1","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_13","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_28","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_15","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_15","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_29","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>RESNET50</p> edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_3","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/image_classification/resnet50/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/image_classification/resnet50/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_16","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_16","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_16","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_30","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_15","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_15","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/image_classification/resnet50/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/image_classification/resnet50/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/image_classification/resnet50/#native-environment_17","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_17","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_17","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_31","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"<p>RESNET50</p> edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_4","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_8","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_14","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_32","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_16","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_16","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_32","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_18","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_18","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_33","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_17","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_17","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_33","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_15","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_34","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_18","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_18","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_34","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_19","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_19","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_35","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_19","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#multistream_19","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_35","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_9","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_16","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_16","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_36","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_16","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_36","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_20","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_20","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_20","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=1000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_37","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_17","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_37","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/image_classification/resnet50/#docker-environment_17","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/image_classification/resnet50/#docker-container-build-and-performance-estimation-for-offline-scenario_17","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_38","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_18","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_38","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#native-environment_21","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#setup-a-virtual-environment-for-python_21","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#performance-estimation-for-offline-scenario_21","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=5000\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#offline_39","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#server_19","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_39","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/","title":"Question Answering using Bert-Large","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"benchmarks/language/bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/bert/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_18","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#deepsparse-framework_2","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_19","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>  Please click here to view available generic model stubs for bert deepsparse <ul> <li> <p>obert-large-pruned95_quant-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p> </li> <li> <p>mobilebert-none-14layer_pruned50_quant-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p> </li> <li> <p>mobilebert-none-base_quant-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p> </li> <li> <p>bert-base-pruned95_obs_quant-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p> </li> <li> <p>mobilebert-none-14layer_pruned50-none-vnni: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p> </li> <li> <p>obert-base-pruned90-none: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>obert-large-pruned97_quant-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p> </li> <li> <p>bert-base-pruned90-none: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p> </li> <li> <p>bert-large-pruned80_quant-none-vnni: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p> </li> <li> <p>obert-large-pruned95-none-vnni: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p> </li> <li> <p>obert-large-pruned97-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p> </li> <li> <p>bert-large-base-none: zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>obert-large-base-none: zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p> </li> <li> <p>mobilebert-none-base-none: zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for bert-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/bert/#offline_20","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/bert/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_1","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/bert/#offline_21","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_22","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/bert/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/bert/#offline_23","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_2","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_24","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_8","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_16","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_25","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_9","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_26","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_16","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_17","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_27","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_17","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_8","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/bert/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_18","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_28","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_18","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_19","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_29","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_19","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_3","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_20","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_30","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#singlestream_10","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#datacenter-category_6","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_16","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/bert/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_21","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_31","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_20","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_7","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework_2","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device_2","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/bert/#native-environment_17","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for bert-99.</p>"},{"location":"benchmarks/language/bert/#performance-estimation-for-offline-scenario_22","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/bert/#offline_32","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/bert/#server_21","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_32","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/get-bert-data/","title":"Question Answering using Bert-Large","text":""},{"location":"benchmarks/language/get-bert-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>BERT validation run uses the SQuAD v1.1 dataset.</p>"},{"location":"benchmarks/language/get-bert-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,squad,validation -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Bert-Large Model</p> PytorchOnnxTensorflow"},{"location":"benchmarks/language/get-bert-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_onnx -j\n</code></pre>"},{"location":"benchmarks/language/get-bert-data/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/language/get-gptj-data/","title":"Text Summarization using GPT-J","text":""},{"location":"benchmarks/language/get-gptj-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>GPT-J validation run uses the CNNDM dataset.</p>"},{"location":"benchmarks/language/get-gptj-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,cnndm,validation -j\n</code></pre>"},{"location":"benchmarks/language/get-gptj-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf GPT-J Model</p> Pytorch"},{"location":"benchmarks/language/get-gptj-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,gptj,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/","title":"Text Summarization using LLAMA2-70b","text":""},{"location":"benchmarks/language/get-llama2-70b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p>"},{"location":"benchmarks/language/get-llama2-70b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,openorca,validation -j\n</code></pre>"},{"location":"benchmarks/language/get-llama2-70b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf LLAMA2-70b Model</p> Pytorch <p>Tip</p> <p>Downloading llama2-70B model from Hugging Face will prompt you to enter the Hugging Face username and password. Please note that the password required is the access token generated for your account. Additionally, ensure that your account has access to the llama2-70B model.</p>"},{"location":"benchmarks/language/get-llama2-70b-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,llama2-70b,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/","title":"Get mixtral 8x7b data","text":""},{"location":"benchmarks/language/get-mixtral-8x7b-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the preprocessed validation and calibration datasets. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>mixtral-8x7b validation run uses the combined dataset - Open ORCA, GSM8K and MBXP.</p>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset-mixtral,openorca-mbxp-gsm8k-combined -j\n</code></pre>"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf MIXTRAL-8x7b Model</p> Pytorch"},{"location":"benchmarks/language/get-mixtral-8x7b-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,mixtral -j\n</code></pre>"},{"location":"benchmarks/language/gpt-j/","title":"Text Summarization using GPT-J","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"benchmarks/language/gpt-j/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>GPTJ-99.9</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_1","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_8","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_9","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB(fp32). 40GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_18","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/gpt-j/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for gptj-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_19","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> <li><code>--beam-size=1</code> Beam size of 4 is mandatory for a closed division submission but reducing the beam size can help in running the model on GPUs with lower device memory</li> </ul>"},{"location":"benchmarks/language/gpt-j/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category_2","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_20","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_10","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>GPTJ-99.9</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#offline_21","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_3","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_22","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_11","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/gpt-j/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for gptj-99.</p>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_23","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category_4","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/gpt-j/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_24","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_12","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_25","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#singlestream_13","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/gpt-j/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/gpt-j/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_26","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/gpt-j/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#offline_27","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/gpt-j/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/gpt-j/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<pre><code>WIP\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/","title":"Text Summarization using LLAMA2-70b","text":"MLCommons-PythonNvidiaNeural MagicAMD"},{"location":"benchmarks/language/llama2-70b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8x80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8x80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for llama2-70b-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> <li> <p>The dataset for NVIDIA's implementation of Llama2 is not publicly available. The user must fill this form and be verified as a MLCommons member to access the dataset.</p> </li> <li> <p><code>PATH_TO_PICKE_FILE</code> should be replaced with path to the downloaded pickle file.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKLE_FILE&gt;\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 80GB</p> </li> <li> <p>Disk Space: 700GB</p> </li> </ul> Docker"},{"location":"benchmarks/language/llama2-70b/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --tp_size=2 \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKLE_FILE&gt;\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --tp_size=&lt;TP_SIZE&gt; \\\n   --nvidia_llama2_dataset_file_path=&lt;PATH_TO_PICKE_FILE&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#neural-magic-mlperf-implementation","title":"Neural Magic MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_2","title":"pytorch framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/llama2-70b/#run-the-inference-server","title":"# Run the Inference Server","text":"<pre><code>cm run script --tags=run,vllm-server \\\n --model=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --quiet\n</code></pre> <p>Tip</p> <ul> <li>Host and Port number of the server can be configured through <code>--host</code> and <code>--port</code> options. Otherwise, server will run on the default host <code>localhost</code> and port <code>8000</code>.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#run-the-inference-server_1","title":"# Run the Inference Server","text":"<pre><code>cm run script --tags=run,vllm-server \\\n --model=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n --quiet\n</code></pre> <p>Tip</p> <ul> <li>Host and Port number of the server can be configured through <code>--host</code> and <code>--port</code> options. Otherwise, server will run on the default host <code>localhost</code> and port <code>8000</code>.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_3","title":"pytorch framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> DockerNative"},{"location":"benchmarks/language/llama2-70b/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=neuralmagic \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --api_server=http://localhost:8000 \\\n   --vllm_model_name=nm-testing/Llama-2-70b-chat-hf-FP8 \\\n   --adr.mlperf-implementation.tags=_repo.https://github.com/neuralmagic/inference,_branch.vllm\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#amd-mlperf-implementation","title":"AMD MLPerf Implementation","text":"<p>LLAMA2-70B-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_6","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_4","title":"pytorch framework","text":"cuda"},{"location":"benchmarks/language/llama2-70b/#cuda-device_6","title":"cuda device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>LLAMA2-70B-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_16","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_7","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_5","title":"pytorch framework","text":"cuda"},{"location":"benchmarks/language/llama2-70b/#cuda-device_7","title":"cuda device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 700GB</li> </ul> Native"},{"location":"benchmarks/language/llama2-70b/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for llama2-70b-99.</p>"},{"location":"benchmarks/language/llama2-70b/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#server_17","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=amd \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/","title":"Question Answering, Math, and Code Generation using Mixtral-8x7B","text":"MLCommons-Python"},{"location":"benchmarks/language/mixtral-8x7b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>MIXTRAL-8X7B</p> datacenter"},{"location":"benchmarks/language/mixtral-8x7b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, mixtral-8x7b has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/mixtral-8x7b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/mixtral-8x7b/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 4x80GB</p> </li> <li> <p>Disk Space: 100GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/mixtral-8x7b/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 100GB</li> </ul> Native"},{"location":"benchmarks/language/mixtral-8x7b/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/mixtral-8x7b/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for mixtral-8x7b you can follow this README.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/mixtral-8x7b/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/mixtral-8x7b/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=mixtral-8x7b \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/","title":"Question and Answering using Bert Large for IndySCC 2024","text":""},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#introduction","title":"Introduction","text":"<p>This guide is designed for the IndySCC 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Bert Large across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Bert Large requires processing a minimum of 10833 samples in both performance and accuracy modes using the Squad v1.1 dataset.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#scoring","title":"Scoring","text":"<p>In the IndySCC 2024, your objective will be to run a reference (unoptimized) Python implementation of the MLPerf inference benchmark to complete a successful submission passing the submission checker. Only one of the available framework needs to be submitted.</p> <p>Info</p> <p>Both MLPerf and CM automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>All the needed files are automatically pushed to the GitHub repository if you manage to complete the given commands. No additional files need to be submitted.</p> MLCommons-Python"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>BERT-99</p> edge"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> PytorchDeepsparse"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \\\n   --env.CM_MLPERF_NEURALMAGIC_MODEL_ZOO_STUB=zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none\n</code></pre>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#submission-commands","title":"Submission Commands","text":""},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=edge \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"benchmarks/language/reproducibility/indyscc24-bert/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference.</p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>cm run script --tags=push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":"MLCommons-PythonNvidiaIntel"},{"location":"benchmarks/medical_imaging/3d-unet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>3D-UNET-99</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_9","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_8","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_9","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_12","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_13","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_14","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_18","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_15","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for 3d-unet-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_19","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>3D-UNET-99</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_20","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_10","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_21","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_16","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_22","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_11","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_17","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_23","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>3D-UNET-99</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_24","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_12","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_25","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_13","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_26","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>3D-UNET-99.9</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_27","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_5","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_6","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_20","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_28","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_14","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_21","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_29","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_15","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_7","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/medical_imaging/3d-unet/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_22","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_30","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for 3d-unet-99.</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#performance-estimation-for-offline-scenario_23","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_31","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":""},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>3d-unet validation run uses the KiTS19 dataset performing KiTS 2019 kidney tumor segmentation task</p>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#get-validation-datasetoriginal","title":"Get Validation Dataset(Original)","text":"<pre><code>cm run script --tags=get,dataset,kits19,_validation -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#get-validation-datasetpreprocessed","title":"Get Validation Dataset(Preprocessed)","text":"<pre><code>cm run script --tags=get,dataset,kits19,preprocessed -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf 3d-unet Model</p> PytorchOnnxTensorflow"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_onnx -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/get-3d-unet-data/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/","title":"Object Detection using Retinanet","text":""},{"location":"benchmarks/object_detection/get-retinanet-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>Retinanet validation run uses the OpenImages v6 MLPerf validation dataset resized to 800x800 and consisting of 24,576 images.</p> <p>Retinanet calibration dataset consist of 500 images selected from the OpenImages v6 dataset.</p> <pre><code>cm run script --tags=get,dataset,openimages,_calibration -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,openimages,_validation -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Retinanet Model</p> PytorchOnnx"},{"location":"benchmarks/object_detection/get-retinanet-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/object_detection/get-retinanet-data/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_onnx -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/","title":"Object Detection using Retinanet","text":"MLCommons-PythonNvidiaIntelQualcommMLCommons-C++"},{"location":"benchmarks/object_detection/retinanet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>RETINANET</p> edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_1","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_2","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_3","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_4","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_5","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_6","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_7","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_8","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_8","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_9","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_9","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_2","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_8","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_8","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_8","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_9","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_9","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_16","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_16","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_17","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_17","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_10","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_10","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_10","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_18","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_18","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_3","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_11","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_11","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_11","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for retinanet you can follow this README.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_19","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_19","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>RETINANET</p> edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_4","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> Docker"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_8","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_8","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_20","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_10","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_10","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_20","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_5","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 8GB</p> </li> <li> <p>Disk Space: 200GB</p> </li> </ul> Docker"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_9","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_9","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_21","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_21","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>RETINANET</p> edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_4","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_10","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_10","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_22","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_11","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_11","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_22","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_12","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_12","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_12","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_23","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_12","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_12","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_23","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_5","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_11","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_11","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_24","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_24","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_13","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_13","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_13","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_25","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0 \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_25","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.0,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>RETINANET</p> edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_14","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_14","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_14","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_26","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_13","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_13","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_26","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device_1","title":"QAIC device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> Native"},{"location":"benchmarks/object_detection/retinanet/#native-environment_15","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_15","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_15","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=qaic  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_27","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_27","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=qaic \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"<p>RETINANET</p> edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, MultiStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_6","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_12","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_12","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_28","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_14","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_14","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_28","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_16","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_16","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_16","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_29","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_15","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_15","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_29","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_6","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_13","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_13","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_30","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_16","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_16","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_30","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_17","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_17","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_17","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamMultiStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_31","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_17","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#multistream_17","title":"MultiStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=MultiStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_31","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_7","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_14","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_14","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_32","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_32","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_18","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_18","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_18","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=100\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_33","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_33","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_7","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 200GB</li> </ul> DockerNative"},{"location":"benchmarks/object_detection/retinanet/#docker-environment_15","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/object_detection/retinanet/#docker-container-build-and-performance-estimation-for-offline-scenario_15","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=500\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_34","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_16","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_34","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#native-environment_19","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#setup-a-virtual-environment-for-python_19","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#performance-estimation-for-offline-scenario_19","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=500\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#offline_35","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#server_17","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_35","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/","title":"Recommendation using DLRM v2","text":"MLCommons-PythonNvidiaIntel"},{"location":"benchmarks/recommendation/dlrm-v2/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>DLRM-V2-99</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_3","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_4","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 2x80GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_5","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_6","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_8","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Native"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_7","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for dlrm-v2-99.9 you can follow this README.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_9","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>DLRM-V2-99</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_10","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB</p> </li> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> Docker"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_8","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50 \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_11","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet  \\\n   --criteo_day23_raw_data_path=&lt;PATH_TO_CRITEO_DAY23_RAW_DATA&gt;\n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>DLRM-V2-99</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_12","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_9","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <p>DLRM-V2-99.9</p> datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_13","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, dlrm-v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Disk Space: 500GB</p> </li> <li> <p>System Memory(RAM+SWAP): 512GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/recommendation/dlrm-v2/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_10","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_14","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>You can reuse the same environment as described for dlrm-v2-99.</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#performance-estimation-for-offline-scenario_11","title":"Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_15","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=dlrm-v2-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/","title":"Recommendation using DLRM v2","text":""},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>DLRM validation run uses the Criteo dataset (Day 23).</p>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,criteo,_validation -j\n</code></pre>"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf DLRM v2 Model</p> Pytorch"},{"location":"benchmarks/recommendation/get-dlrm-v2-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,dlrm,_pytorch -j\n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/","title":"Text to Image using Stable Diffusion","text":""},{"location":"benchmarks/text_to_image/get-sdxl-data/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>Stable Diffusion validation run uses the Coco 2014 dataset.</p>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,coco2014,_validation -j\n</code></pre>"},{"location":"benchmarks/text_to_image/get-sdxl-data/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Stable Diffusion Model</p> Pytorch"},{"location":"benchmarks/text_to_image/get-sdxl-data/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,sdxl,_pytorch -j\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/","title":"Text to Image using Stable Diffusion","text":"MLCommons-PythonNvidiaIntel"},{"location":"benchmarks/text_to_image/sdxl/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>SDXL</p> edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_1","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_1","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_2","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_2","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_3","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_3","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_2","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_2","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_2","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_4","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_4","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_4","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_1","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_2","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_2","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_5","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_5","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_3","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_3","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_3","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_6","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_1","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_6","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=bfloat16</code> can give better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_3","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_3","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_7","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_2","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_7","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_4","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_4","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_4","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n   --test_query_count=50\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_8","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_3","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_8","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device_1","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_5","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_5","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_5","title":"# Performance Estimation for Offline Scenario","text":"<pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul> <p>The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul> <ul> <li>If you want to download the official MLPerf model and dataset for sdxl you can follow this README.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_9","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_4","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_9","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=rocm \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> <li><code>--precision=float16</code> can help run on GPUs with less RAM / gives better performance </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_1","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_2","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_4","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_4","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_10","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_5","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_10","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_3","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_5","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_5","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> <li> <p>Default batch size is assigned based on GPU memory or the specified GPU. Please click more option for docker launch or run command to see how to specify the GPU name.</p> </li> <li> <p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=50\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_11","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#server_5","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_11","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cuda \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>Tip</p> <ul> <li>Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.</li> </ul> <p>SDXL</p> edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_2","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_2","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_6","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_6","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_12","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_6","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_12","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_6","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_6","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_6","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineSingleStreamAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_13","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_7","title":"SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_13","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_3","title":"CPU device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/sdxl/#docker-environment_7","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#docker-container-build-and-performance-estimation-for-offline-scenario_7","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li> <p><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </p> </li> <li> <p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p> </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=10\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_14","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#server_6","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_14","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#native-environment_7","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/sdxl/#setup-a-virtual-environment-for-python_7","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#performance-estimation-for-offline-scenario_7","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cpu  \\\n   --quiet \\\n   --test_query_count=10\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> OfflineServerAll Scenarios <p>  Please click here to see more options for the RUN command <ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists </p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#offline_15","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#server_7","title":"Server","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server\\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_15","title":"All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --server_target_qps=&lt;SERVER_TARGET_QPS&gt; \\\n   --execution_mode=valid \\\n   --device=cpu \\\n   --quiet \n</code></pre> <p>Tip</p> <ul> <li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/","title":"Text-to-Image with Stable Diffusion for Student Cluster Competition 2024","text":""},{"location":"benchmarks/text_to_image/reproducibility/scc24/#introduction","title":"Introduction","text":"<p>This guide is designed for the Student Cluster Competition 2024 to walk participants through running and optimizing the MLPerf Inference Benchmark using Stable Diffusion XL 1.0 across various software and hardware configurations. The goal is to maximize system throughput (measured in samples per second) without compromising accuracy. Since the model performs poorly on CPUs, it is essential to run it on GPUs.</p> <p>For a valid MLPerf inference submission, two types of runs are required: a performance run and an accuracy run. In this competition, we focus on the <code>Offline</code> scenario, where throughput is the key metric\u2014higher values are better. The official MLPerf inference benchmark for Stable Diffusion XL requires processing a minimum of 5,000 samples in both performance and accuracy modes using the COCO 2014 dataset. However, for SCC, we have reduced this and we also have two variants. <code>scc-base</code> variant has dataset size reduced to 50 samples, making it possible to complete both performance and accuracy runs in approximately 5-10 minutes. <code>scc-main</code> variant has dataset size of 500 and running it will fetch extra points as compared to running just the base variant. Setting up for Nvidia GPUs may take 2-3 hours but can be done offline. Your final output will be a tarball (<code>mlperf_submission.tar.gz</code>) containing MLPerf-compatible results, which you will submit to the SCC organizers for scoring.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#scoring","title":"Scoring","text":"<p>In the SCC, your first objective will be to run <code>scc-base</code> variant for reference (unoptimized) Python implementation or a vendor-provided version (such as Nvidia's) of the MLPerf inference benchmark to secure a baseline score.</p> <p>Once the initial run is successful, you'll have the opportunity to optimize the benchmark further by maximizing system utilization, applying quantization techniques, adjusting ML frameworks, experimenting with batch sizes, and more, all of which can earn you additional points.</p> <p>Since vendor implementations of the MLPerf inference benchmark vary and are often limited to single-node benchmarking, teams will compete within their respective hardware categories (e.g., Nvidia GPUs, AMD GPUs). Points will be awarded based on the throughput achieved on your system.</p> <p>Additionally, significant bonus points will be awarded if your team enhances an existing implementation, adds support for new hardware (such as an unsupported GPU), enables multi-node execution, or adds/extends scripts to cm4mlops repository supporting new devices, frameworks, implementations etc. All improvements must be made publicly available under the Apache 2.0 license and submitted alongside your results to the SCC committee to earn these bonus points, contributing to the MLPerf community.</p> <p>Info</p> <p>Both MLPerf and CM automation are evolving projects. If you encounter issues or have questions, please submit them here</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#artifacts-to-submit-to-the-scc-committee","title":"Artifacts to submit to the SCC committee","text":"<p>You will need to submit the following files:</p> <ul> <li><code>mlperf_submission.run</code> - CM commands to run MLPerf inference benchmark saved to this file.</li> <li><code>mlperf_submission.md</code> - description of your platform and some highlights of the MLPerf benchmark execution.</li> <li><code>&lt;Team Name&gt;</code> under which results are pushed to the github repository. </li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#scc-interview","title":"SCC interview","text":"<p>You are encouraged to highlight and explain the obtained MLPerf inference throughput on your system and describe any improvements and extensions to this benchmark (such as adding new hardware backend or supporting multi-node execution) useful for the community and MLCommons.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#run-commands","title":"Run Commands","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>Tip</p> <ul> <li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li> </ul> <p>SDXL</p> datacenter"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#pytorch-framework","title":"Pytorch framework","text":"ROCmCUDA"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#rocm-device","title":"ROCm device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li>Disk Space: 50GB</li> </ul> Native"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#native-environment","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm  \\\n   --quiet \\\n    --precision=float16\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=rocm \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#cuda-device","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 24GB(fp32), 16GB(fp16)</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> DockerNative"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-environment","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n    --precision=float16\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> Offline"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_1","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#native-environment_1","title":"Native Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p> <p>Tip</p> <ul> <li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#setup-a-virtual-environment-for-python_1","title":"# Setup a virtual environment for Python","text":"<pre><code>cm run script --tags=install,python-venv --name=mlperf\nexport CM_SCRIPT_EXTRA_CMD=\"--adr.python.name=mlperf\"\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#performance-estimation-for-offline-scenario_1","title":"# Performance Estimation for Offline Scenario","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --quiet \\\n    --precision=float16\n</code></pre> The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p> Offline"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_2","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet --precision=float16\n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>SDXL</p> datacenter"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#cuda-device_1","title":"CUDA device","text":"<p> Please click here to see the minimum system requirements for running the benchmark <ul> <li> <p>Device Memory: 16GB</p> </li> <li> <p>Disk Space: 50GB</p> </li> </ul> Docker"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-environment_1","title":"Docker Environment","text":"<p>Please refer to the installation page to install CM for running the automated benchmark commands.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#docker-container-build-and-performance-estimation-for-offline-scenario_1","title":"# Docker Container Build and Performance Estimation for Offline Scenario","text":"<p>Tip</p> <ul> <li><code>--env.CM_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes</code> option can be used to download the model on the host so that it can be reused across different container lanuches. </li> </ul> <p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda  \\\n   --docker --quiet\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for the Offline scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo=&lt;Custom CM GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cm_repo_branch=&lt;Custom CM GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in CM are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used. </li> </ul> Offline <p>Info</p> <p>Once the above run is successful, you can change <code>_scc24-base</code> to <code>_scc24-main</code> to run the main variant.</p>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#offline_3","title":"Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution_mode=test \\\n   --device=cuda \\\n   --quiet \n</code></pre>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#submission-commands","title":"Submission Commands","text":""},{"location":"benchmarks/text_to_image/reproducibility/scc24/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --run_style=test \\\n   --adr.submission-checker.tags=_short-run \\\n   --quiet \\\n   --submitter=&lt;Team Name&gt;\n</code></pre> <ul> <li>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name.</li> </ul>"},{"location":"benchmarks/text_to_image/reproducibility/scc24/#push-results-to-github","title":"Push Results to GitHub","text":"<p>Fork the <code>mlperf-inference-results-scc24</code> branch of the repository URL at https://github.com/mlcommons/cm4mlperf-inference. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub fork URL.</p> <pre><code>cm run script --tags=push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/&lt;myfork&gt;/cm4mlperf-inference \\\n   --repo_branch=mlperf-inference-results-scc24 \\\n   --commit_message=\"Results on system &lt;HW Name&gt;\" \\\n   --quiet\n</code></pre> <p>Once uploaded give a Pull Request to the origin repository. Github action will be running there and once  finished you can see your submitted results at https://docs.mlcommons.org/cm4mlperf-inference.</p>"},{"location":"changelog/","title":"What's New, What's Coming","text":""},{"location":"changelog/changelog/","title":"Release Notes","text":""},{"location":"demos/","title":"Demos","text":""},{"location":"install/","title":"Installation","text":"<p>We use MLCommons CM Automation framework to run MLPerf inference benchmarks.</p> <p>CM needs <code>git</code>, <code>python3-pip</code> and <code>python3-venv</code> installed on your system. If any of these are absent, please follow the official CM installation page to install them. Once the dependencies are installed, do the following</p>"},{"location":"install/#activate-a-virtual-env-for-cm","title":"Activate a Virtual ENV for CM","text":"<p>This step is not mandatory as CM can use separate virtual environment for MLPerf inference. But the latest <code>pip</code> install requires this or else will need the <code>--break-system-packages</code> flag while installing <code>cm4mlops</code>.</p> <pre><code>python3 -m venv cm\nsource cm/bin/activate\n</code></pre>"},{"location":"install/#install-cm-and-pulls-any-needed-repositories","title":"Install CM and pulls any needed repositories","text":"Use the default fork of CM MLOps repositoryUse custom fork/branch of the CM MLOps repository <pre><code> pip install cm4mlops\n</code></pre> <p><pre><code> pip install cmind &amp;&amp; cm init --quiet --repo=mlcommons@cm4mlops --branch=mlperf-inference\n</code></pre> Here, <code>repo</code> is in the format <code>githubUsername@githubRepo</code>.</p> <p>Now, you are ready to use the <code>cm</code> commands to run MLPerf inference as given in the benchmarks page</p>"},{"location":"submission/","title":"Submission Generation","text":"CM based benchmarkNon CM based benchmark <p>If you have followed the <code>cm run</code> commands under the individual model pages in the benchmarks directory, all the valid results will get aggregated to the <code>cm cache</code> folder. The following command could be used to browse the structure of inference results folder generated by CM.</p> <p>If you have not followed the <code>cm run</code> commands under the individual model pages in the benchmarks directory, please make sure that the result directory is structured in the following way.  <pre><code>\u2514\u2500\u2500 System description ID(SUT Name)\n    \u251c\u2500\u2500 system_meta.json\n    \u2514\u2500\u2500 Benchmark\n        \u2514\u2500\u2500 Scenario\n            \u251c\u2500\u2500 Performance\n            |   \u2514\u2500\u2500 run_x/#1 run for all scenarios\n            |       \u251c\u2500\u2500 mlperf_log_summary.txt\n            |       \u2514\u2500\u2500 mlperf_log_detail.txt\n            \u251c\u2500\u2500 Accuracy\n            |   \u251c\u2500\u2500 mlperf_log_summary.txt\n            |   \u251c\u2500\u2500 mlperf_log_detail.txt\n            |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n            |   \u2514\u2500\u2500 accuracy.txt\n            \u2514\u2500\u2500 Compliance_Test_ID\n                \u251c\u2500\u2500 Performance\n                |   \u2514\u2500\u2500 run_x/#1 run for all scenarios\n                |       \u251c\u2500\u2500 mlperf_log_summary.txt\n                |       \u2514\u2500\u2500 mlperf_log_detail.txt\n                \u251c\u2500\u2500 Accuracy\n                |   \u251c\u2500\u2500 baseline_accuracy.txt\n                |   \u251c\u2500\u2500 compliance_accuracy.txt\n                |   \u251c\u2500\u2500 mlperf_log_accuracy.json\n                |   \u2514\u2500\u2500 accuracy.txt\n                \u251c\u2500\u2500 verify_performance.txt\n                \u2514\u2500\u2500 verify_accuracy.txt #for TEST01 only\n</code></pre></p> <p> Click here if you are submitting in open division <ul> <li>The <code>model_mapping.json</code> should be included inside the SUT folder which is used to map the custom model full name to the official model name. The format of json file is:</li> </ul> <p><pre><code>    {\n        \"custom_model_name_for_model1\":\"official_model_name_for_model1\",\n        \"custom_model_name_for_model2\":\"official_model_name_for_model2\",\n\n    }\n</code></pre> </p> <p>Once all the results across all the models are ready you can use the following command to generate a valid submission tree compliant with the MLPerf requirements.</p>"},{"location":"submission/#get-results-folder-structure","title":"Get results folder structure","text":"<pre><code>cm find cache --tags=get,mlperf,inference,results,dir | xargs tree\n</code></pre>"},{"location":"submission/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"Closed EdgeClosed DatacenterOpen EdgeOpen Datacenter <ul> <li> <p>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name. Examples can be seen here</p> </li> <li> <p>Use <code>--submitter=&lt;Your name&gt;</code> if your organization is an official MLCommons member and would like to submit under your organization</p> </li> <li> <p>Use <code>--hw_notes_extra</code> option to add additional notes like <code>--hw_notes_extra=\"Result taken by NAME\"</code></p> </li> <li> <p>Use <code>--results_dir</code> option to specify the results folder for Non CM based benchmarks</p> </li> </ul> <p>The above command should generate \"submission.tar.gz\" if there are no submission checker issues and you can upload it to the MLCommons Submission UI.</p>"},{"location":"submission/#closed-edge-submission","title":"Closed Edge Submission","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --submitter=MLCommons \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=closed \\\n   --category=edge \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --quiet\n</code></pre>"},{"location":"submission/#closed-datacenter-submission","title":"Closed Datacenter Submission","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --submitter=MLCommons \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=closed \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --quiet\n</code></pre>"},{"location":"submission/#open-edge-submission","title":"Open Edge Submission","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --submitter=MLCommons \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=edge \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --quiet\n</code></pre>"},{"location":"submission/#closed-datacenter-submission_1","title":"Closed Datacenter Submission","text":"<pre><code>cm run script --tags=generate,inference,submission \\\n   --clean \\\n   --preprocess_submission=yes \\\n   --run-checker \\\n   --submitter=MLCommons \\\n   --tar=yes \\\n   --env.CM_TAR_OUTFILE=submission.tar.gz \\\n   --division=open \\\n   --category=datacenter \\\n   --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n   --quiet\n</code></pre>"},{"location":"submission/#aggregate-results-in-github","title":"Aggregate Results in GitHub","text":"<p>If you are collecting results across multiple systems you can generate different submissions and aggregate all of them to a GitHub repository (can be private) and use it to generate a single tar ball which can be uploaded to the MLCommons Submission UI. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub repository URL.</p> <pre><code>cm run script --tags=push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/GATEOverflow/mlperf_inference_submissions_v4.1 \\\n   --commit_message=\"Results on &lt;HW name&gt; added by &lt;Name&gt;\" \\\n   --quiet\n</code></pre> <p>At the end, you can download the github repo and upload to the MLCommons Submission UI.</p>"},{"location":"usage/","title":"Using CM for MLPerf Inference","text":""}]}